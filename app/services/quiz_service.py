"""
Quiz service integrating existing quiz functionality with progress tracking.
"""
from typing import Dict, Any, List, Optional
import json

from app.services.progress_service import ProgressService
from app.quiz.service import build_context_from_search, generate_mcqs_llm_with_error, validate_mcqs

# Global quiz cache - in production, use Redis or database
_quiz_cache = {}

class QuizService:
    """Service for health fact quizzes."""
    
    def __init__(self, progress_service: ProgressService):
        self.progress_service = progress_service
    
    def generate_quiz_from_claim(self, claim: str, user_id: int) -> Dict[str, Any]:
        """
        Generate a quiz from a health claim using existing quiz infrastructure.
        """
        try:
            # Use existing context building
            contexts = build_context_from_search(claim, top_urls=4, chars=1200)
            
            if not contexts:
                return {
                    "claim": claim,
                    "questions": [],
                    "quiz_id": f"quiz_{user_id}_{hash(claim) % 10000}",
                    "error": f"No reliable sources found for claim: '{claim}'. Try a more specific health-related claim."
                }
            
            # Use existing LLM quiz generation
            items, error = generate_mcqs_llm_with_error(
                contexts, claim, n=3, difficulty="medium", style="health"
            )
            
            if error:
                return {
                    "claim": claim,
                    "questions": [],
                    "quiz_id": f"quiz_{user_id}_{hash(claim) % 10000}",
                    "error": f"Quiz generation failed: {error}"
                }
            
            # Validate questions
            validated = validate_mcqs(items, contexts)
            
            if not validated:
                # Better debugging
                debug_info = []
                if not items:
                    debug_info.append("No questions were generated by LLM")
                else:
                    debug_info.append(f"LLM generated {len(items)} questions but validation failed")
                    for i, item in enumerate(items):
                        issues = []
                        if not isinstance(item.get("options"), list) or len(item.get("options", [])) != 4:
                            issues.append("invalid_options")
                        if item.get("correct_index") not in [0, 1, 2, 3]:
                            issues.append("invalid_correct_index")
                        if item.get("source_url") not in [c["url"] for c in contexts]:
                            issues.append("invalid_source_url")
                        debug_info.append(f"Question {i+1}: {', '.join(issues) if issues else 'unknown_issue'}")
                
                return {
                    "claim": claim,
                    "questions": [],
                    "quiz_id": f"quiz_{user_id}_{hash(claim) % 10000}",
                    "error": f"No valid questions could be generated. Debug: {'; '.join(debug_info)}"
                }
            
            # Format questions for API response
            formatted_questions = []
            for i, item in enumerate(validated):
                formatted_questions.append({
                    "id": i + 1,
                    "question": item["question"],
                    "options": item["options"],
                    "explanation": item["explanation"],
                    "source_url": item["source_url"]
                })
            
            quiz = {
                "claim": claim,
                "questions": formatted_questions,
                "quiz_id": f"quiz_{user_id}_{hash(claim) % 10000}",
                "contexts": contexts  # Store contexts for grading
            }
            
            # Store quiz data in global cache for grading
            quiz_cache_key = f"{quiz['quiz_id']}_{user_id}"
            _quiz_cache[quiz_cache_key] = quiz
            
            # Track progress for quiz generation
            source_url = contexts[0]["url"] if contexts else None
            self.progress_service.add_quiz_fact(
                user_id, 
                claim, 
                source_url=source_url, 
                questions_count=len(formatted_questions)
            )
            
            return quiz
            
        except Exception as e:
            return {
                "claim": claim,
                "questions": [],
                "quiz_id": f"quiz_{user_id}_{hash(claim) % 10000}",
                "error": f"Unexpected error: {str(e)}"
            }
    
    def grade_quiz(self, quiz_id: str, answers: List[str], user_id: int, 
                   quiz_data: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Grade a completed quiz using existing validation logic.
        """
        try:
            # Store quiz data in a simple in-memory cache for grading
            # In production, this would be stored in database or Redis
            quiz_cache_key = f"{quiz_id}_{user_id}"
            
            if not quiz_data:
                # Try to get quiz data from global cache
                quiz_data = _quiz_cache.get(quiz_cache_key)
                
            if not quiz_data:
                # If still no quiz data, we can't grade properly
                return {
                    "quiz_id": quiz_id,
                    "score": 0,
                    "total_questions": len(answers),
                    "score_percentage": 0,
                    "passed": False,
                    "results": [],
                    "error": "Quiz data not found - cannot grade"
                }
            
            # Universal grading logic - compare by finding user answer in options
            questions = quiz_data.get("questions", [])
            correct_count = 0
            results = []
            
            for i, question in enumerate(questions):
                if i >= len(answers):
                    break
                    
                user_answer = answers[i]
                options = question.get("options", [])
                correct_index = question.get("correct_index", 0)
                
                # Find which option index the user selected
                user_index = -1
                for j, option in enumerate(options):
                    if option.strip() == user_answer.strip():
                        user_index = j
                        break
                
                # Check if user selected the correct option
                is_correct = user_index == correct_index
                if is_correct:
                    correct_count += 1
                
                # Get the actual correct answer text
                correct_answer = options[correct_index] if correct_index < len(options) else "Unknown"
                
                results.append({
                    "question_number": i + 1,
                    "user_answer": user_answer,
                    "correct_answer": correct_answer,
                    "is_correct": is_correct,
                    "user_index": user_index,
                    "correct_index": correct_index
                })
                

            
            total_questions = len(results)
            score_percentage = (correct_count / total_questions) * 100 if total_questions > 0 else 0
            
            grading_result = {
                "quiz_id": quiz_id,
                "score": correct_count,
                "total_questions": total_questions,
                "score_percentage": score_percentage,
                "passed": score_percentage >= 60,  # 60% passing grade
                "results": results
            }
            
            # Track progress only for correct answers
            correct_user_answers = []
            correct_expected_answers = []
            
            for result in results:
                if result["is_correct"]:
                    correct_user_answers.append(result["user_answer"])
                    correct_expected_answers.append(result["correct_answer"])
            
            # Only track progress if there are correct answers
            if correct_user_answers:
                self.progress_service.add_quiz_answers(user_id, correct_user_answers, correct_expected_answers)
            
            return grading_result
            
        except Exception as e:
            return {
                "quiz_id": quiz_id,
                "score": 0,
                "total_questions": len(answers),
                "score_percentage": 0.0,
                "passed": False,
                "results": [],
                "error": f"Grading failed: {str(e)}"
            }
    
    def get_quiz_history(self, user_id: int, limit: int = 10) -> List[Dict[str, Any]]:
        """Get user's quiz history."""
        progress = self.progress_service.get_user_progress(user_id)
        if not progress:
            return []
        
        # Get user facts and filter for quiz type
        user = self.progress_service.user_repository.get_by_id(user_id)
        if not user:
            return []
        
        facts = user.facts_as_list
        quiz_facts = [
            fact for fact in facts 
            if fact.get("type") in ["quiz", "quiz_answer"]
        ]
        
        # Sort by learned_at descending and limit
        quiz_facts.sort(key=lambda x: x.get("learned_at", ""), reverse=True)
        return quiz_facts[:limit]
